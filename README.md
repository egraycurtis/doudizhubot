# Overview

This project implements a DQN and trains it to play dou dizhu. Previously I built doudizhu.online to play family and friends and built this to be able to play when we can't find a third, also to learn something. The doudizhu.online repo is private but happy to add you if you shoot me an email.

doudizhu.online is currently down - it used a postgres instance from elephantsql which no longer exists... rip. Could definitely get it back up pretty easily but havent had the urge to play.

# Model

I experimented with several different models. Initally, I tried a CNN with custom filters on the convolutional layer, thinking filters designed to identify hands (eg three of a kind, straights etc) would be ideal, but a MLP with custom features proved to be more effective and much simpler. 

The most successful model ended up being a transformer to evaluate the previous 15 moves, who's output was then fed into an MLP (fully connected 512 -> 256 -> 128 -> 64) along with the game state. The output is a single sigmoid activated node representing the probability of winning given the game state.

The game has three distinct roles based on position relative to the landlord. Initially, I tried to account for this in the game state fed into the model, but I found a separate model for each position performed the best (especially when it came to teamwork between the non landlords).

# Features / Game State

All the tensors contain just 1s and 0s.

The deck contains 54 cards including the jokers. Twos are played high with value between ace and joker.

A 1x54 tensor of a hand -> [1, 1, 1, 0, 1, 0, 0, 0... 0] would translate to three 3s and a 4 

## Transformer input

a 15x54 tensor representing the previous 15 moves (54 because the the deck includes the two jokers)

## MLP input

the output from the transformer

a few 1x54 tensors (move choice, cards remaining in hand after move choice, cards person to the left has played, cards person to the right has played, and all cards not seen)

two 1x85 tensors which identified other moves (straights, run of pairs etc) both in the hand and in the cards not seen

one 1x2 tensor identifying who played last

two 1x5 tensor showing how many cards each player has left.

# Training

The bottleneck for training was generating the training data by self play. A large part of this project was optimizing the games per second. This can always be improved by upping resources / training cluster approach but I stuck with my M1 and got it going fast enough where it became as good as me after letting it run for a couple weeks. The main speed improvements were due to:

## Running games in parallel / Batching predictions

Runs 50 games simulatiously in a core (could be tweaked depending on cpu/gpu but games per second stopped improving past 50 on the M1 mac). Each game does one move, then each option from each of the 50 would be combined and batched through keras. The outputs are then used to make the next move and so on.

## Multiprocessing

Runs the self play on n-2 cores. At the end of a game, it applies the update function and uploads the data to a redis cache. 

One core reads from this redis cache and trains / updates the models. 

One core is used to evaluate the progress by playing against a fixed model.

# Rules
I built this game based on our house rules. It's mostly the same rules you'll find on [wikipedia](https://en.wikipedia.org/wiki/Dou_dizhu) with the following exceptions:

**Start:** One card is cut face up into the deck before dealing. Whoever is dealt the face up card has first option to take the middle cards to become the landlord. If you have 3 trump cards (jokers, 2s, or bombs), you must take the middle cards. If neither of the first two players take the cards, the third player takes them blind (middle cards are not revealed) and the stakes are doubled.

**Hands:** Pairs are not valid discards with three of a kind or bombs eg 33344 and AAAA3355 are invaild.

# Project Structure

**main.py** - Trains the bot. Will run train.py and compete.py on one core each and self_play.py on the remaining.

**self_play.py** - Runs continuous self-play games, generating data to train the bots.

**train.py** - Processes training data collected from self-play, updating neural network models to improve gameplay strategies.

**transfer.py** - Tranfer learning. Uses data generated by one model to train another.

**bot.py** - This connects to the doudizhu.online db and alows the bot to play againt people online.

**compete.py** - Compares trained models with self play. Tried out a bunch of different architechtures, learning rates etc and used this to evaluate.

**models/transformer/transformermodel.py** - Final architechture for the model I landed on.

# Installation

clone this repo

docker compose up

(python 3.10) pip install -r requirements.txt

python main.py (or whatever)
